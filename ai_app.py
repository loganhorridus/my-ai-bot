import os
import streamlit as st
import google.generativeai as genai

# ==========================================
# 1. åŸºç¡€é…ç½® (è®°å¾—æŠŠ Key æ”¹æˆä½ è‡ªå·±çš„ï¼Œæˆ–è€…ç”¨ st.secrets)
# ==========================================
# å¼ºåˆ¶ä»£ç† (ä½ çš„ 15236 ç«¯å£)
os.environ["http_proxy"] = "http://127.0.0.1:15236"
os.environ["https_proxy"] = "http://127.0.0.1:15236"

# è¯»å– Key (è¿™é‡Œå‡è®¾ä½ å·²ç»é…å¥½äº† secrets.tomlï¼Œå¦‚æœæ²¡æœ‰ï¼Œæš‚æ—¶æŠŠä¸‹é¢è¿™è¡Œæ”¹æˆä½ çš„ 'AIzaSy...')
# ç›´æ¥å¡«å…¥ä½ çš„ API Key (æ³¨æ„ä¿ç•™å¼•å·)
genai.configure(api_key="AIzaSyDb8Na3JA88ukXL86ztgcPpHF4uZrsB0ZQ")

st.set_page_config(page_title="è¶…çº§ AI åŠ©æ‰‹ 2.0", page_icon="ğŸ“‚", layout="wide")

# ==========================================
# 2. ä¾§è¾¹æ ï¼šæ§åˆ¶å° & æ–‡ä»¶ä¸Šä¼ 
# ==========================================
with st.sidebar:
    st.title("ğŸ›ï¸ æŠ•å–‚åŒº")
    
    # ğŸŒŸ æ–°åŠŸèƒ½ï¼šæ–‡ä»¶ä¸Šä¼ å™¨
    uploaded_file = st.file_uploader("ä¸Šä¼ ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ (.txt/.md/.py)", type=["txt", "md", "py"])
    
    # å¦‚æœç”¨æˆ·ä¸Šä¼ äº†æ–‡ä»¶ï¼Œè¯»å–å†…å®¹
    file_content = ""
    if uploaded_file is not None:
        # è¯»å–æ–‡ä»¶å†…å®¹å¹¶è§£ç ä¸ºä¸­æ–‡
        try:
            file_content = uploaded_file.read().decode("utf-8")
            st.success(f"âœ… å·²è¯»å–æ–‡ä»¶: {uploaded_file.name}")
            with st.expander("æŸ¥çœ‹æ–‡ä»¶å†…å®¹é¢„è§ˆ"):
                st.text(file_content[:500] + "...") # åªæ˜¾ç¤ºå‰500å­—é¢„è§ˆ
        except Exception as e:
            st.error("æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œè¯·ç¡®ä¿æ˜¯çº¯æ–‡æœ¬æ–‡ä»¶ã€‚")

    st.divider()
    
    # è§’è‰²é€‰æ‹©
    role = st.selectbox("é€‰æ‹© AI è§’è‰²", ["å°è¯´ç»­å†™åŠ©æ‰‹", "ä»£ç å®¡æŸ¥å‘˜", "é€šç”¨åŠ©æ‰‹"])
    
    # åˆ›é€ åŠ›è°ƒèŠ‚
    temperature = st.slider("è„‘æ´ç¨‹åº¦", 0.0, 2.0, 0.7)
    
    # æ¸…ç©ºæŒ‰é’®
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯"):
        st.session_state.messages = []
        st.rerun()

# ==========================================
# 3. èŠå¤©ä¸»é€»è¾‘
# ==========================================
st.title(f"ğŸ“‚ æˆ‘çš„ AI åŠ©æ‰‹ - {role}")

if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºå†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# å¤„ç†ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„æŒ‡ä»¤..."):
    
    # ğŸŒŸ å…³é”®ä¿®æ”¹ï¼šå¦‚æœæœ‰æ–‡ä»¶ï¼ŒæŠŠæ–‡ä»¶å†…å®¹â€œå·å·â€å¡ç»™ AI
    final_prompt = prompt
    if file_content:
        # è¿™æ˜¯ä¸€ä¸ªâ€œæç¤ºè¯å·¥ç¨‹â€æŠ€å·§ï¼šæŠŠèµ„æ–™åŒ…è£…å¥½å–‚ç»™ AI
        final_prompt = f"""
        ã€èƒŒæ™¯èµ„æ–™ã€‘ï¼š
        {file_content}
        
        ã€ç”¨æˆ·æŒ‡ä»¤ã€‘ï¼š
        {prompt}
        """
    
    # 1. æ˜¾ç¤ºç”¨æˆ·çš„é—®é¢˜ (ç•Œé¢ä¸Šåªæ˜¾ç¤ºç®€æ´çš„é—®é¢˜)
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 2. è°ƒç”¨ AI
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            # å®ä¾‹åŒ–æ¨¡å‹
            model = genai.GenerativeModel(
                'models/gemini-2.5-flash',
                # ç³»ç»ŸæŒ‡ä»¤ï¼šå‘Šè¯‰ AI å®ƒçš„èº«ä»½
                system_instruction=f"ä½ ç°åœ¨çš„èº«ä»½æ˜¯ï¼š{role}ã€‚è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„èƒŒæ™¯èµ„æ–™ï¼ˆå¦‚æœæœ‰ï¼‰æ¥å›ç­”é—®é¢˜ã€‚",
                generation_config=genai.types.GenerationConfig(temperature=temperature)
            )
            
            # å‘é€æ‹¼æ¥åçš„ prompt (åŒ…å«æ–‡ä»¶å†…å®¹)
            response = model.generate_content(final_prompt, stream=True)
            
            for chunk in response:
                if chunk.text:
                    full_response += chunk.text
                    message_placeholder.markdown(full_response + "â–Œ")
            
            message_placeholder.markdown(full_response)
            st.session_state.messages.append({"role": "assistant", "content": full_response})
            
        except Exception as e:
            st.error(f"å‡ºé”™å•¦: {str(e)}")